import os
import json
import time
import re
import google.generativeai as genai
from pathlib import Path
from PIL import Image

class ImageOCR:
    def __init__(self, kq_dir: str, output_file: str, metadata_file: str, api_key: str):
        self.kq_dir = kq_dir
        self.output_file = output_file
        self.metadata_file = metadata_file
        self.metadata = self._load_metadata()
        self.image_counter = 0
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('models/gemini-2.5-flash')
    
    def _extract_slug_from_url(self, url: str) -> str:
        """Extract slug from URL"""
        path = url.rstrip('/').split('/')[-1].replace('.html', '')
        return re.sub(r'-\d+$', '', path)[:100]
    
    def _load_metadata(self) -> dict:
        """Load metadata to map folder -> URL"""
        url_map = {}
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                if isinstance(data, dict) and "results" in data:
                    data = data["results"]
                elif isinstance(data, dict):
                    data = [data]
                
                for item in data:
                    if not isinstance(item, dict):
                        continue
                    
                    url = item.get("url")
                    if not url:
                        continue
                    
                    # Multiple mapping strategies
                    slug = self._extract_slug_from_url(url)
                    url_map[slug] = url
                    
                    for key in ["file_hash", "filename", "file", "slug", "id"]:
                        if item.get(key):
                            clean_key = str(item[key]).replace(".txt", "")
                            url_map[clean_key] = url
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not load metadata - {e}")
        
        return url_map
    
    def _is_image(self, filename: str) -> bool:
        """Check if file is an image"""
        extensions = {'.png', '.jpg', '.jpeg', '.webp', '.gif', '.bmp'}
        return Path(filename).suffix.lower() in extensions
    
    def _detect_and_ocr(self, image_path: str) -> tuple:
        """
        Detect type, OCR image, and summarize for RAG.
        Returns: (img_type, full_content, summary_content)
        """
        try:
            img = Image.open(image_path)
            
            # === B∆Ø·ªöC 1: PH√ÇN LO·∫†I ·∫¢NH ===
            detect_prompt = """Ph√¢n lo·∫°i ·∫£nh n√†y:
- N·∫øu c√≥ B·∫¢NG (table) v·ªõi nhi·ªÅu h√†ng/c·ªôt ‚Üí tr·∫£ l·ªùi: "table"
- N·∫øu ch·ªâ c√≥ TEXT th√¥ng th∆∞·ªùng (brochure, document) ‚Üí tr·∫£ l·ªùi: "text"
Ch·ªâ tr·∫£ l·ªùi 1 t·ª´: "table" ho·∫∑c "text"
"""
            
            detection = self.model.generate_content([detect_prompt, img])
            img_type = detection.text.strip().lower()
            
            # === B∆Ø·ªöC 2: TR√çCH XU·∫§T N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶ (FULL CONTENT) ===
            if 'table' in img_type:
                ocr_prompt = """Tr√≠ch xu·∫•t B·∫¢NG n√†y th√†nh text c√≥ c·∫•u tr√∫c r√µ r√†ng.
Format y√™u c·∫ßu:
- D√≤ng ƒë·∫ßu: T√™n c·ªôt 1 | T√™n c·ªôt 2 | T√™n c·ªôt 3
- C√°c d√≤ng sau: Gi√° tr·ªã 1 | Gi√° tr·ªã 2 | Gi√° tr·ªã 3
QUAN TR·ªåNG: Gi·ªØ nguy√™n vƒÉn b·∫£n ti·∫øng Vi·ªát. Kh√¥ng th√™m gi·∫£i th√≠ch. Ch·ªâ xu·∫•t b·∫£ng."""
                result_type = "table"
            else:
                ocr_prompt = """Tr√≠ch xu·∫•t T·∫§T C·∫¢ vƒÉn b·∫£n t·ª´ ·∫£nh n√†y.
Y√™u c·∫ßu:
- Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng (ti√™u ƒë·ªÅ, bullet points)
- Gi·ªØ nguy√™n ti·∫øng Vi·ªát, kh√¥ng b·ªè s√≥t th√¥ng tin.
- Kh√¥ng th√™m gi·∫£i th√≠ch. Ch·ªâ xu·∫•t vƒÉn b·∫£n g·ªëc."""
                result_type = "text"
            
            response = self.model.generate_content([ocr_prompt, img])
            full_content = response.text.strip()
            
            # Clean AI-generated prefixes
            unwanted_prefixes = ["D∆∞·ªõi ƒë√¢y l√†", "ƒê√¢y l√†", "N·ªôi dung", "B·∫£ng", "Here is"]
            for prefix in unwanted_prefixes:
                if full_content.lower().startswith(prefix.lower()):
                    newline_idx = full_content.find('\n')
                    if newline_idx > 0:
                        full_content = full_content[newline_idx+1:].strip()
                    break
            
            if not full_content:
                img.close()
                return result_type, None, None

            # === B∆Ø·ªöC 3: T√ìM T·∫ÆT N·ªòI DUNG (SUMMARY CONTENT) ===
            summary_content = ""
            if len(full_content) < 300: # N·∫øu qu√° ng·∫Øn, d√πng lu√¥n n·ªôi dung g·ªëc
                summary_content = full_content
            else:
                try:
                    summarize_prompt = f"""T√≥m t·∫Øt n·ªôi dung sau th√†nh 1-2 c√¢u m√¥ t·∫£ ng·∫Øn g·ªçn.
M·ª•c ƒë√≠ch l√† ƒë·ªÉ t√¨m ki·∫øm (embedding), kh√¥ng ph·∫£i ƒë·ªÉ tr·∫£ l·ªùi.
V√≠ d·ª•: "B·∫£ng h·ªçc ph√≠ nƒÉm 2024" ho·∫∑c "Th√¥ng b√°o 7 b∆∞·ªõc nh·∫≠p h·ªçc".

N·ªôi dung c·∫ßn t√≥m t·∫Øt:
{full_content[:2000]}... 
"""
                    summary_response = self.model.generate_content(summarize_prompt)
                    summary_content = summary_response.text.strip()
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  L·ªói khi t√≥m t·∫Øt: {e}")
                    summary_content = full_content[:200] # Fallback

            # === B∆Ø·ªöC 4: L√ÄM S·∫†CH B·∫¢N T√ìM T·∫ÆT ===
            # Lo·∫°i b·ªè c√°c c√¢u "chat" c·ªßa AI m√† b·∫°n ƒë√£ ph√°t hi·ªán
            ai_noise_patterns = [
                re.compile(r"ƒê√¢y l√† m·ªôt s·ªë l·ª±a ch·ªçn t√≥m t·∫Øt.*?:", re.IGNORECASE),
                re.compile(r"D∆∞·ªõi ƒë√¢y l√† b·∫£n t√≥m t·∫Øt.*?:", re.IGNORECASE),
                re.compile(r"^\s*-\s*", re.MULTILINE), # X√≥a c√°c bullet point ·ªü ƒë·∫ßu
                re.compile(r"[\"']", re.MULTILINE) # X√≥a d·∫•u ngo·∫∑c k√©p/ƒë∆°n
            ]
            
            for pattern in ai_noise_patterns:
                summary_content = pattern.sub("", summary_content).strip()

            # N·∫øu sau khi l√†m s·∫°ch m√† t√≥m t·∫Øt b·ªã r·ªóng, d√πng fallback
            if not summary_content:
                summary_content = full_content[:200] # Fallback: l·∫•y 200 k√Ω t·ª± ƒë·∫ßu

            img.close()
            return result_type, full_content, summary_content
            
        except Exception as e:
            print(f"    ‚ùå Error: {e}")
            return None, None, None
    
    def _get_next_order(self) -> int:
        """Get next order number from existing chunks"""
        if not os.path.exists(self.output_file):
            return 1
        
        max_order = 0
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        chunk = json.loads(line)
                        order = chunk.get('metadata', {}).get('order', 0)
                        if not order:
                            order = chunk.get('order', 0)
                        max_order = max(max_order, order)
        except:
            pass
        
        return max_order + 1
    
    def process_all(self):
        """Scan and OCR all images"""
        mode = 'a' if os.path.exists(self.output_file) else 'w'
        next_order = self._get_next_order()
        
        print(f"üìã Loaded {len(self.metadata)} URL mappings\n")
        
        with open(self.output_file, mode, encoding='utf-8') as jsonl_file:
            for folder_name in sorted(os.listdir(self.kq_dir)):
                folder_path = os.path.join(self.kq_dir, folder_name)
                
                if not os.path.isdir(folder_path):
                    continue
                
                images_dir = os.path.join(folder_path, 'images')
                if not os.path.isdir(images_dir):
                    continue
                
                # Get URL for this folder
                file_hash = folder_name.replace('.txt', '')
                url = self.metadata.get(file_hash)
                
                # Fallback: fuzzy match
                if not url:
                    for key, val in self.metadata.items():
                        if file_hash in key or key in file_hash:
                            url = val
                            break
                
                if not url:
                    url = 'unknown'
                    print(f"‚ö†Ô∏è  {folder_name} - No URL found")
                
                print(f"\nüìÅ {folder_name}")
                
                image_files = sorted([f for f in os.listdir(images_dir) 
                                     if self._is_image(f)])
                
                if not image_files:
                    print("    ‚ö†Ô∏è  No images found")
                    continue
                
                for img_file in image_files:
                    img_path = os.path.join(images_dir, img_file)
                    print(f"    üî§ Processing: {img_file}")
                    
                    # L·∫•y c·∫£ 3 gi√° tr·ªã
                    img_type, full_content, summary_content = self._detect_and_ocr(img_path)
                    
                    if not full_content or not summary_content:
                        print("    ‚ùå B·ªè qua (Kh√¥ng c√≥ n·ªôi dung)")
                        continue
                    
                    self.image_counter += 1
                    
                    # One image = One chunk (Summary-RAG)
                    chunk_obj = {
                        'chunk_id': f"{file_hash}_image_{self.image_counter}",
                        'url': url,
                        'content': summary_content, # T√≥m t·∫Øt (cho embedding)
                        'metadata': {
                            'type': f"image_{img_type}", # VD: image_table, image_text
                            'order': next_order,
                            'source_file': img_file,
                            'full_content': full_content # N·ªôi dung ƒë·∫ßy ƒë·ªß (cho LLM)
                        }
                    }
                    
                    jsonl_file.write(json.dumps(chunk_obj, ensure_ascii=False) + '\n')
                    print(f"    ‚úÖ {img_type} - {len(full_content)} chars - {len(summary_content)} chars (summary)")
                    
                    next_order += 1
                    time.sleep(2)  # Rate limiting for Gemini
                
                print(f"    üìä {len(image_files)} images processed")
        
        print(f"\n{'='*60}")
        print(f"‚ú® Total images: {self.image_counter}")
        print(f"üíæ Output: {self.output_file}")
        print(f"{'='*60}")


if __name__ == "__main__":
    
    API_KEY = "AIzaSyClYVCbxN1B2IKsDeUmu7YS5EyF9923fqo" 
    
    KQ_DIR = r"C:\Users\nguye\OneDrive\Desktop\New folder (2)\raw_data\KQ"
    OUTPUT_FILE = r"C:\Users\nguye\OneDrive\Desktop\New folder (2)\chunk_data\chunks.jsonl"
    METADATA_FILE = r"C:\Users\nguye\OneDrive\Desktop\New folder (2)\raw_data\KQ\metadata.json"
    
    print(f"{'='*60}\nüñºÔ∏è  OCR Images with Gemini 2.5 Flash (Summary-RAG)\n{'='*60}\n")
    
    if "AIzaSy" not in API_KEY:
        print("‚ùå Vui l√≤ng c·∫≠p nh·∫≠t API_KEY")
    else:
        try:
            ocr = ImageOCR(KQ_DIR, OUTPUT_FILE, METADATA_FILE, API_KEY)
            ocr.process_all()
            print("\n‚úÖ Done!")
        except Exception as e:
            print(f"‚ùå Fatal error: {e}")
            import traceback
            traceback.print_exc()